# Apache Airflow Setup Guide

## ðŸŽ¯ What is Apache Airflow?

Apache Airflow is the **industry-standard workflow orchestration platform** used by:
- Netflix (data processing)
- Airbnb (data pipelines)
- Bloomberg (financial data)
- PayPal (transaction processing)

It transforms our basic Python scripts into **professionally orchestrated DAGs** (Directed Acyclic Graphs).

## ðŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APACHE AIRFLOW WEB UI                   â”‚
â”‚         http://localhost:8090                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ DAG 1    â”‚ â”‚ DAG 2    â”‚ â”‚ DAG 3    â”‚        â”‚
â”‚  â”‚ Graph    â”‚ â”‚ Events   â”‚ â”‚ Correlateâ”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AIRFLOW SCHEDULER (Task Execution)           â”‚
â”‚  âœ“ Retry logic                                  â”‚
â”‚  âœ“ Dependency management                        â”‚
â”‚  âœ“ SLA monitoring                               â”‚
â”‚  âœ“ Parallel execution                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         YOUR LANGGRAPH PIPELINES                â”‚
â”‚  (Claude AI + Neo4j + PostgreSQL)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ What We Created

### 1. Airflow Infrastructure

**[`docker-compose-airflow.yml`](./docker-compose-airflow.yml)** - Full Airflow stack:
- Webserver (UI on port 8090)
- Scheduler (DAG execution)
- Database (uses existing PostgreSQL)
- Automatic initialization

### 2. Four Production DAGs

**[`dags/data_ingestion_dag.py`](./dags/data_ingestion_dag.py)** - Every minute
- Fetches OHLCV from Yahoo Finance
- Parallel writes to PostgreSQL, Redis, Neo4j
- 25 stocks tracked

**[`dags/company_graph_dag.py`](./dags/company_graph_dag.py)** - Hourly
- Fetches company data
- Claude identifies competitors
- Builds Neo4j relationship graph
- 30 companies analyzed

**[`dags/events_tracker_dag.py`](./dags/events_tracker_dag.py)** - Every 5 minutes
- Fetches company news
- Claude classifies event types
- Creates MarketEvent nodes
- Sentiment analysis

**[`dags/correlation_analyzer_dag.py`](./dags/correlation_analyzer_dag.py)** - Hourly
- Fetches 30-day price history
- Calculates correlation matrix
- Claude explains correlations
- Creates CORRELATED_WITH edges

## ðŸš€ Quick Start

### Step 1: Create Required Directories

```bash
cd /home/sandeep/pertinent/axiom/axiom/pipelines/airflow

# Create Airflow directories
mkdir -p logs plugins

# Set permissions
chmod 777 logs plugins
```

### Step 2: Initialize Airflow Database

```bash
# Create airflow database in PostgreSQL
psql -h localhost -U axiom -d postgres -c "CREATE DATABASE airflow;"
```

### Step 3: Start Airflow

```bash
# From airflow directory
docker compose -f docker-compose-airflow.yml up -d

# Watch initialization
docker logs -f axiom-airflow-init
```

### Step 4: Access Airflow UI

1. Open browser: http://localhost:8090
2. Login:
   - Username: `admin`
   - Password: `admin123`

3. You'll see 4 DAGs:
   - `data_ingestion` (â° every minute)
   - `company_graph_builder` (â° hourly)
   - `events_tracker` (â° every 5 minutes)
   - `correlation_analyzer` (â° hourly)

### Step 5: Enable DAGs

In the Airflow UI:
1. Toggle each DAG to "ON"
2. Watch them execute automatically
3. View task logs by clicking on task boxes

## ðŸ“Š What You Get

### Professional Monitoring

**DAG View**:
- Visual representation of task dependencies
- Color-coded task status (green = success, red = failure)
- Click any task to see logs

**Grid View**:
- Historical run matrix
- Spot patterns in failures
- Track execution times

**Calendar View**:
- See when DAGs ran
- Identify gaps or issues

### Automatic Features

**Built-in Retries**:
```python
# Configured in each DAG
'retries': 3,
'retry_delay': timedelta(minutes=5)
```

**SLA Monitoring**:
```python
# Alerts if takes too long
'sla': timedelta(minutes=45)
```

**Email Alerts**:
```python
'email_on_failure': True
```

**Backfill Support**:
```bash
# Reprocess historical dates
airflow dags backfill company_graph_builder \
  --start-date 2025-11-01 \
  --end-date 2025-11-15
```

## ðŸŽ›ï¸ Common Operations

### View All DAGs

```bash
docker exec axiom-airflow-webserver airflow dags list
```

### Trigger DAG Manually

```bash
docker exec axiom-airflow-webserver \
  airflow dags trigger company_graph_builder
```

### View DAG Run Status

```bash
docker exec axiom-airflow-webserver \
  airflow dags list-runs -d company_graph_builder
```

### Pause/Unpause DAG

```bash
# Pause
docker exec axiom-airflow-webserver \
  airflow dags pause company_graph_builder

# Unpause
docker exec axiom-airflow-webserver \
  airflow dags unpause company_graph_builder
```

## ðŸ” Monitoring & Debugging

### Check Scheduler Health

```bash
docker logs axiom-airflow-scheduler --tail 50
```

### View DAG Logs

```bash
# In Airflow UI:
# 1. Click on DAG
# 2. Click on specific run (green/red box)
# 3. Click on task
# 4. Click "Log" button
```

### Check Task XCom Data

```bash
# In Airflow UI:
# 1. Click on task
# 2. Click "XCom" tab
# 3. See data passed between tasks
```

## ðŸ†š Comparison: Before vs After

### Before (Basic Docker)

```python
# Simple while loop
while True:
    process_data()
    await asyncio.sleep(60)
```

**Issues**:
- âŒ No visibility into failures
- âŒ No retry logic
- âŒ Hard to change schedules
- âŒ No task dependencies
- âŒ No monitoring

### After (Airflow)

```python
# Professional DAG
with DAG(
    'company_graph_builder',
    schedule_interval='@hourly',
    default_args={'retries': 3}
) as dag:
    fetch >> analyze >> store >> validate
```

**Benefits**:
- âœ… Visual DAG monitoring
- âœ… Automatic retries
- âœ… Easy schedule changes (just edit cron)
- âœ… Clear task dependencies
- âœ… Built-in monitoring
- âœ… SLA alerts
- âœ… Backfill capability
- âœ… Parallel task execution

## ðŸŽ¯ Production Features

### 1. Task Dependencies

Tasks automatically wait for upstream tasks:
```python
fetch >> analyze >> store  # Linear
fetch >> [postgres, redis, neo4j]  # Parallel
```

### 2. Automatic Retries

If a task fails, Airflow retries automatically:
```python
'retries': 3,
'retry_delay': timedelta(minutes=5)
```

### 3. SLA Monitoring

Alert if DAG takes too long:
```python
'sla': timedelta(minutes=45)
# If exceeds 45 minutes, send alert
```

### 4. Execution Timeout

Kill runaway tasks:
```python
'execution_timeout': timedelta(minutes=30)
# Task killed if exceeds 30 minutes
```

### 5. Max Active Runs

Prevent concurrent runs:
```python
'max_active_runs': 1
# Only one instance runs at a time
```

## ðŸ“ˆ Next Steps

### This Week: Get Comfortable with Airflow

1. âœ… Start Airflow
2. âœ… Enable all DAGs
3. âœ… Watch them execute
4. âœ… Explore the UI
5. âœ… View task logs

### Next Week: Advanced Features

1. Add email alerts (configure SMTP)
2. Create custom operators
3. Set up Airflow variables/connections
4. Add data quality checks

### Month 1: Kafka Integration

1. Deploy Kafka + Zookeeper
2. Convert to event-driven architecture
3. Add streaming topics
4. Multiple consumers

### Month 2: Ray Parallel Processing

1. Deploy Ray cluster
2. Parallel company analysis
3. GPU-accelerated models
4. 10x throughput increase

## ðŸ”§ Troubleshooting

### Airflow Won't Start

```bash
# Check logs
docker logs axiom-airflow-init

# Common issue: Database not ready
# Solution: Ensure PostgreSQL is running first
docker compose -f ../../database/docker-compose.yml ps
```

### DAGs Not Showing Up

```bash
# Check DAG directory is mounted
docker exec axiom-airflow-webserver ls /opt/airflow/dags

# Refresh DAGs
docker exec axiom-airflow-webserver airflow dags list-import-errors
```

### Tasks Failing

1. Check task logs in UI
2. Verify environment variables
3. Test database connectivity
4. Check Claude API key

## ðŸ’° Cost Implications

**Airflow itself**: Free (open source)
**Resources**: Minimal overhead (~100MB RAM for scheduler)
**Claude API**: Same as before (we just orchestrate it better)

**ROI**: 
- 10x reduction in debugging time
- Professional monitoring
- Automatic recovery
- Production-ready

## ðŸ“š Learning Resources

**Official Docs**: https://airflow.apache.org/docs/
**Course**: "Data Engineering with Apache Airflow" on Udemy
**Examples**: /opt/airflow/dags contains our 4 production DAGs

## âœ… Success Criteria

You'll know Airflow is working when:

1. âœ… UI accessible at http://localhost:8090
2. âœ… All 4 DAGs visible and enabled
3. âœ… DAGs running on schedule (see grid view)
4. âœ… Task logs show successful execution
5. âœ… Neo4j graph growing with relationships

## ðŸŽ‰ Congratulations!

You've just upgraded from basic scripts to **professional enterprise orchestration** used by Fortune 500 companies!

Next up: Kafka streaming for real-time event processing. ðŸš€