# Centralized DAG Configuration
# All schedules, thresholds, and parameters are configurable here

# ================================================================
# Global Settings
# ================================================================
global:
  owner: axiom
  email:
    - admin@axiom.com
  email_on_failure: false  # SMTP not configured
  email_on_retry: false
  catchup: false
  max_active_runs: 1
  
  # Default retry settings
  retries: 3
  retry_delay_minutes: 5
  
  # Database connections (environment variables will be used at runtime)
  postgres:
    host_env: POSTGRES_HOST
    user_env: POSTGRES_USER
    password_env: POSTGRES_PASSWORD
    database_env: POSTGRES_DB
  
  redis:
    host_env: REDIS_HOST
    password_env: REDIS_PASSWORD
  
  neo4j:
    uri_env: NEO4J_URI
    user_env: NEO4J_USER
    password_env: NEO4J_PASSWORD

# ================================================================
# Market Data Symbols (shared across DAGs)
# ================================================================
symbols:
  primary:
    - AAPL
    - MSFT
    - GOOGL
    - TSLA
    - NVDA
    - AMZN
    - META
    - NFLX
  
  extended:
    - GOOG
    - CRM
    - ORCL
    - ADBE
    - INTC
    - AMD
    - QCOM
    - AVGO
    - CSCO
    - TXN
    - UBER
    - LYFT
    - SNAP
    - JPM
    - BAC
    - GS
    - MS
  
  top_50:
    # Batch 1: Mega-Cap Tech (10)
    - AAPL     # Apple - Consumer Electronics
    - MSFT     # Microsoft - Software
    - GOOGL    # Alphabet - Internet
    - AMZN     # Amazon - E-commerce
    - NVDA     # NVIDIA - Semiconductors
    - META     # Meta - Social Media
    - TSLA     # Tesla - EV/Energy
    - BRK.B    # Berkshire - Conglomerate
    - UNH      # UnitedHealth - Healthcare
    - JPM      # JPMorgan - Banking
    
    # Batch 2: Large-Cap Tech & Payments (10)
    - V        # Visa - Payments
    - MA       # Mastercard - Payments
    - HD       # Home Depot - Retail
    - PG       # Procter & Gamble - Consumer Goods
    - DIS      # Disney - Media
    - NFLX     # Netflix - Streaming
    - ADBE     # Adobe - Software
    - CRM      # Salesforce - Cloud
    - ORCL     # Oracle - Enterprise Software
    - INTC     # Intel - Semiconductors
    
    # Batch 3: Diversified Large-Cap (10)
    - PFE      # Pfizer - Pharmaceuticals
    - MRK      # Merck - Pharmaceuticals
    - ABBV     # AbbVie - Biopharmaceuticals
    - KO       # Coca-Cola - Beverages
    - PEP      # PepsiCo - Food & Beverage
    - WMT      # Walmart - Retail
    - COST     # Costco - Retail
    - CVX      # Chevron - Energy
    - XOM      # Exxon - Energy
    - BA       # Boeing - Aerospace
    
    # Batch 4: Financial & Industrial (10)
    - GS       # Goldman Sachs - Investment Banking
    - MS       # Morgan Stanley - Investment Banking
    - BAC      # Bank of America - Banking
    - WFC      # Wells Fargo - Banking
    - C        # Citigroup - Banking
    - AXP      # American Express - Financial Services
    - BLK      # BlackRock - Asset Management
    - CAT      # Caterpillar - Industrial
    - DE       # Deere - Agricultural Equipment
    - MMM      # 3M - Conglomerate
    
    # Batch 5: Tech, Healthcare, Telecom (10)
    - AMD      # AMD - Semiconductors
    - QCOM     # Qualcomm - Wireless Tech
    - AVGO     # Broadcom - Semiconductors
    - CSCO     # Cisco - Networking
    - IBM      # IBM - IT Services
    - JNJ      # Johnson & Johnson - Pharmaceuticals
    - LLY      # Eli Lilly - Pharmaceuticals
    - TMO      # Thermo Fisher - Life Sciences
    - VZ       # Verizon - Telecom
    - T        # AT&T - Telecom

# ================================================================
# Data Ingestion DAG Configuration
# ================================================================
data_ingestion:
  dag_id: data_ingestion_v2
  description: "Real-time data with multi-source failover - NO per-trigger validation"
  
  # Schedule: Every minute for real-time ingestion
  schedule_interval: "*/1 * * * *"
  execution_timeout_minutes: 5
  
  # Multi-source failover configuration
  data_sources:
    primary: yahoo
    fallback:
      - polygon
      - finnhub
  
  # Circuit breaker settings
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout_seconds: 60
  
  # Cache settings
  cache_ttl_minutes: 5
  
  # Storage settings
  parallel_storage: true  # Store to PostgreSQL, Redis, Neo4j simultaneously
  
  # REMOVED: trigger_validation (moved to batch validation)
  # NO per-trigger validation - batch validation handles this now
  
  tags:
    - v2
    - enterprise
    - real-time
    - multi-source
    - failover

# ================================================================
# Data Quality Validation DAG Configuration
# BATCH VALIDATION: Processes 5-minute windows
# ================================================================
data_quality_validation:
  dag_id: data_quality_validation
  description: "Batch validation of 5-minute windows (no per-trigger)"
  
  # Schedule: Every 5 minutes for batch validation
  schedule_interval: "*/5 * * * *"
  execution_timeout_minutes: 10
  
  # Batch validation settings
  batch:
    enabled: true
    window_minutes: 5  # Validate 5-minute windows instead of per-trigger
    min_records_to_validate: 1  # Only validate if at least 1 new record
  
  # Validation thresholds
  thresholds:
    data_freshness_minutes: 120  # Data must be < 2 hours old
    min_symbols_with_recent_data: 10
    max_duplicate_tolerance: 0
    price_min: 0.01
    price_max: 100000
    volume_max: 1000000000000
    max_intraday_movement_percent: 50
  
  # Quality checks configuration
  quality_checks:
    record_level:
      - high_low_check
      - close_within_range
      - positive_values
      - reasonable_movement
      - timestamp_validity
    
    database_level:
      - data_freshness
      - symbol_completeness
      - no_duplicates
      - price_reasonableness
    
    sql_based:
      - hourly_data_count
      - no_stale_data
      - volume_sanity
  
  # Alert settings
  alerts:
    enabled: true
    fail_on_error: false  # Log warnings, don't fail DAG
    min_success_rate_percent: 95
  
  tags:
    - quality
    - validation
    - batch
    - 5-min-windows

# ================================================================
# Company Graph Builder DAG Configuration
# ================================================================
company_graph_builder:
  dag_id: company_graph_builder_v2
  description: "Build company graph with enterprise operators"
  
  # Schedule: Every 5 minutes for active monitoring
  schedule_interval: "*/5 * * * *"
  execution_timeout_minutes: 30
  
  # Claude caching for cost optimization
  claude:
    cache_ttl_hours: 24
    track_cost: true
    max_tokens: 4096
  
  # Circuit breaker settings
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout_seconds: 300
  
  # Neo4j bulk operations
  neo4j:
    batch_size: 1000
    validation:
      min_nodes: 20
      min_relationships: 50
      check_orphans: true
      fail_on_error: false
  
  # Use extended symbol list for comprehensive graph
  symbols_list: extended
  
  tags:
    - v2
    - enterprise
    - claude-cached
    - neo4j
    - cost-optimized

# ================================================================
# Correlation Analyzer DAG Configuration
# ================================================================
correlation_analyzer:
  dag_id: correlation_analyzer_v2
  description: "Correlation analysis with cached explanations"
  
  # Schedule: Every 5 minutes for active monitoring
  schedule_interval: "*/5 * * * *"
  execution_timeout_minutes: 20
  
  # Correlation settings
  correlation:
    lookback_days: 30
    min_data_points: 20
    significance_threshold: 0.5  # Lower threshold for more relationships
    top_n_correlations: 20
  
  # Claude caching (correlations very stable)
  claude:
    cache_ttl_hours: 48  # Extended cache for stable data
    track_cost: true
    max_tokens: 2048
  
  # Circuit breaker settings
  circuit_breaker:
    failure_threshold: 3
    recovery_timeout_seconds: 60
  
  # Data quality requirements
  quality:
    min_recent_symbols: 10
    max_null_percent: 0
  
  # Use primary symbol list
  symbols_list: primary
  
  tags:
    - v2
    - enterprise
    - correlation
    - claude-cached
    - quant

# ================================================================
# Events Tracker DAG Configuration
# ================================================================
events_tracker:
  dag_id: events_tracker_v2
  description: "Track market events with cached Claude classification"
  
  # Schedule: Every 15 minutes (reasonable for news tracking)
  schedule_interval: "*/15 * * * *"
  execution_timeout_minutes: 10
  retries: 2
  retry_delay_minutes: 2
  
  # News fetching settings
  news:
    max_items_per_symbol: 5
    sources:
      - yfinance
  
  # Event classification settings
  event_types:
    - earnings
    - product_launch
    - regulatory
    - acquisition
    - leadership_change
    - other
  
  sentiments:
    - positive
    - negative
    - neutral
  
  impact_levels:
    - high
    - medium
    - low
  
  # Claude caching (news repeated across sources)
  claude:
    cache_ttl_hours: 6
    track_cost: true
    max_tokens: 4096
  
  # Circuit breaker settings
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout_seconds: 120
  
  # Neo4j batch operations
  neo4j:
    batch_create: true
  
  # Use primary symbol list
  symbols_list: primary
  
  tags:
    - v2
    - enterprise
    - events
    - claude-cached
    - cost-optimized

# ================================================================
# Historical Backfill DAG Configuration
# ================================================================
historical_backfill:
  dag_id: historical_backfill
  description: "One-time historical data backfill (1 year daily prices)"
  
  # Schedule: Manual trigger only (not scheduled)
  schedule_interval: null
  execution_timeout_minutes: 60
  retries: 1
  retry_delay_minutes: 10
  
  # Backfill settings
  backfill:
    lookback_days: 365  # 1 year = ~252 trading days
    batch_size: 100
    validate_before_store: true
    fail_on_validation_error: false
  
  # Use extended symbol list (8 stocks)
  symbols_list: extended
  
  tags:
    - backfill
    - historical
    - one-time
    - data-acquisition

# ================================================================
# Data Profiling DAG Configuration
# ================================================================
data_profiling:
  dag_id: data_profiling
  description: "Daily statistical profiling and anomaly detection"
  
  # Schedule: Daily at 1 AM (after midnight cleanup)
  schedule_interval: "0 1 * * *"
  execution_timeout_minutes: 20
  retries: 1
  retry_delay_minutes: 5
  
  # Profiling settings
  profiling:
    sample_size: 1000  # Records to profile
    price_spike_threshold: 0.20  # 20% move triggers alert
    outlier_iqr_multiplier: 1.5
    quality_score_threshold: 95
  
  # Alert settings
  alerts:
    email_on_critical: false  # SMTP not configured yet
    quality_drop_threshold: 10  # Alert if score drops >10 points
  
  tags:
    - quality
    - profiling
    - monitoring
    - anomaly-detection

# ================================================================
# Company Enrichment DAG Configuration
# ================================================================
company_enrichment:
  dag_id: company_enrichment
  description: "Enrich company universe with deep profiles for AI showcase"
  
  # Schedule: Manual trigger (run in batches)
  schedule_interval: null
  execution_timeout_minutes: 30
  retries: 2
  retry_delay_minutes: 5
  
  # Enrichment settings
  enrichment:
    batch_size: 10  # Process 10 companies per run
    total_batches: 5  # 50 companies total
  
  # Claude settings for extraction
  claude:
    cache_ttl_hours: 168  # 7 days (competitors/products stable)
    track_cost: true
    max_tokens: 2048
  
  # Use top 50 companies list
  symbols_list: top_50
  
  tags:
    - enrichment
    - companies
    - ai-showcase
    - text-extraction

# ================================================================
# M&A Deals Ingestion DAG Configuration
# ================================================================
ma_deals_ingestion:
  dag_id: ma_deals_ingestion
  description: "M&A transaction intelligence: scrape, analyze, graph"
  
  # Schedule: Weekly (M&A deals don't change frequently)
  schedule_interval: "@weekly"
  execution_timeout_minutes: 60
  retries: 2
  retry_delay_minutes: 10
  
  # Scraping settings
  scraping:
    batch_size: 50
    rate_limit_delay: 0.5  # seconds between requests
    user_agent: "Axiom M&A Research Bot"
  
  # Claude analysis settings
  claude:
    cache_ttl_hours: 720  # 30 days (deals are historical, don't change)
    track_cost: true
    max_tokens: 4096
  
  tags:
    - ma
    - deals
    - nlp
    - web-scraping
    - ai-showcase

# ================================================================
# Cost Tracking Configuration
# ================================================================
cost_tracking:
  enabled: true
  postgres_table: claude_usage_tracking
  track_metrics:
    - api_calls
    - tokens_input
    - tokens_output
    - cost_per_call
    - cache_hit_rate
    - execution_time

# ================================================================
# Monitoring & Alerting
# ================================================================
monitoring:
  sla_minutes: 45
  
  # Performance thresholds
  performance:
    data_ingestion_max_seconds: 10
    validation_max_seconds: 60
    graph_builder_max_seconds: 300
    correlation_max_seconds: 120
    events_max_seconds: 60
  
  # Success rate thresholds
  success_rates:
    data_ingestion_min_percent: 99.9
    validation_min_percent: 95
    graph_builder_min_percent: 90
    correlation_min_percent: 90
    events_min_percent: 85