# Apache Airflow for Enterprise Pipeline Orchestration
# This provides professional DAG-based workflow management

version: '3.8'

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow
  image: axiom-airflow:2.8.0
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass@localhost:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # Custom environment for our pipelines
    PYTHONPATH: /opt/airflow/axiom:/opt/airflow
    ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    POSTGRES_HOST: localhost
    POSTGRES_USER: ${POSTGRES_USER:-axiom}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-axiom_password}
    POSTGRES_DB: ${POSTGRES_DB:-axiom_finance}
    NEO4J_URI: bolt://localhost:7687
    NEO4J_USER: ${NEO4J_USER:-neo4j}
    NEO4J_PASSWORD: ${NEO4J_PASSWORD:-axiom_neo4j}
    REDIS_HOST: localhost
    REDIS_PASSWORD: ${REDIS_PASSWORD:-axiom_redis}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./operators:/opt/airflow/operators
    - ./dag_factory:/opt/airflow/dag_factory
    - ./dag_configs:/opt/airflow/dag_configs
    - ../../axiom:/opt/airflow/axiom
    - ../../../.env:/opt/airflow/.env
  user: "${AIRFLOW_UID:-50000}:0"
  network_mode: "host"

services:
  # ============================================
  # Airflow Database (uses existing PostgreSQL)
  # ============================================
  
  # ============================================
  # Airflow Webserver (UI)
  # ============================================
  airflow-webserver:
    <<: *airflow-common
    container_name: axiom-airflow-webserver
    command: webserver
    ports:
      - "8090:8080"  # Airflow UI on port 8090 (avoid conflict with health checks)
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    depends_on:
      - airflow-init

  # ============================================
  # Airflow Scheduler
  # ============================================
  airflow-scheduler:
    <<: *airflow-common
    container_name: axiom-airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    depends_on:
      - airflow-init

  # ============================================
  # Airflow Initialization
  # ============================================
  airflow-init:
    <<: *airflow-common
    container_name: axiom-airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Create airflow database if not exists
        psql -h localhost -U axiom -d postgres -c "CREATE DATABASE airflow;" || true
        
        # Initialize Airflow DB
        airflow db init
        
        # Create admin user
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@axiom.com \
          --password admin123 || true
        
        echo "✅ Airflow initialization complete"
    restart: "no"

# ================================================================
# Usage:
# ================================================================
# 
# Start Airflow:
#   cd /home/sandeep/pertinent/axiom/axiom/pipelines/airflow
#   docker compose -f docker-compose-airflow.yml up -d
#
# Access Airflow UI:
#   http://localhost:8090
#   Username: admin
#   Password: admin123
#
# View logs:
#   docker logs -f axiom-airflow-webserver
#   docker logs -f axiom-airflow-scheduler
#
# Stop Airflow:
#   docker compose -f docker-compose-airflow.yml down
#
# ================================================================
# What This Gives You:
# ================================================================
#
# ✅ Professional DAG-based orchestration
# ✅ Web UI for monitoring all pipelines
# ✅ Automatic retries and error handling
# ✅ Task dependency management
# ✅ Backfill support
# ✅ Parallel task execution
# ✅ SLA monitoring
# ✅ Email/Slack alerts on failures
#
# Your LangGraph workflows become Airflow tasks!
# ================================================================