# Data Pipeline Architecture & Workflow

## ğŸ—ï¸ Current Architecture

### Why Only 1 Container?

Currently, we have **1 unified data ingestion pipeline** that handles:
- Real-time market data ingestion
- Multi-database storage (PostgreSQL + Redis + Neo4j)
- Continuous operation (60-second cycles)

This is a **monolithic pipeline design** - one container does everything.

### Alternative: Multi-Container Pipeline Architecture

If you want **multiple specialized pipelines**, we can create:

```yaml
services:
  # 1. Real-time price ingestion (current)
  realtime-prices:
    container_name: axiom-pipeline-realtime
    environment:
      PIPELINE_TYPE: realtime_prices
      SYMBOLS: AAPL,MSFT,GOOGL,TSLA,NVDA
      INTERVAL: 60

  # 2. Historical data backfill
  historical-data:
    container_name: axiom-pipeline-historical
    environment:
      PIPELINE_TYPE: historical_data
      SYMBOLS: AAPL,MSFT,GOOGL
      LOOKBACK_DAYS: 365

  # 3. Company fundamentals
  fundamentals:
    container_name: axiom-pipeline-fundamentals
    environment:
      PIPELINE_TYPE: fundamentals
      SYMBOLS: AAPL,MSFT,GOOGL
      INTERVAL: 3600  # Daily

  # 4. News & sentiment
  news-sentiment:
    container_name: axiom-pipeline-news
    environment:
      PIPELINE_TYPE: news_sentiment
      SOURCES: alpha_vantage,finnhub
      INTERVAL: 300  # 5 minutes

  # 5. Options data
  options-chain:
    container_name: axiom-pipeline-options
    environment:
      PIPELINE_TYPE: options
      SYMBOLS: SPY,QQQ,AAPL,MSFT
      INTERVAL: 60
```

---

## ğŸ”„ Current Pipeline Workflow

### Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LIGHTWEIGHT DATA INGESTION PIPELINE             â”‚
â”‚              (axiom-pipeline-ingestion)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Every 60 Seconds:    â”‚
            â”‚   Run Ingestion Cycle   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 1. Fetch Market Data   â”‚
            â”‚    (yfinance API)      â”‚
            â”‚    - AAPL, MSFT, etc.  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 2. Store in PostgreSQL â”‚
            â”‚    Table: price_data   â”‚
            â”‚    Columns: symbol,    â”‚
            â”‚    timestamp, OHLCV    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 3. Cache in Redis      â”‚
            â”‚    Key: price:{symbol} â”‚
            â”‚    TTL: 60 seconds     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 4. Update Neo4j        â”‚
            â”‚    Node: Stock         â”‚
            â”‚    Properties: price,  â”‚
            â”‚    last_updated        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 5. Log Metrics         â”‚
            â”‚    - Symbols processed â”‚
            â”‚    - Records stored    â”‚
            â”‚    - Errors            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 6. Sleep 60 seconds    â”‚
            â”‚    (await next cycle)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â””â”€â”€â”€â”€ Loop back to step 1
```

---

## ğŸ“‹ Detailed Step-by-Step Workflow

### Step 1: Initialize (One-time, on container start)
```python
pipeline = LightweightPipeline()

# Connects to:
â”œâ”€ PostgreSQL: postgresql://axiom:****@postgres:5432/axiom_db
â”œâ”€ Redis:      redis://redis:6379 (with password)
â””â”€ Neo4j:      bolt://neo4j:7687 (with auth)

# Creates tables if not exist:
â””â”€ price_data (id, symbol, timestamp, open, high, low, close, volume, source)
```

### Step 2: Continuous Loop (Every 60 seconds)
```python
while True:
    # === INGESTION CYCLE START ===
    
    # For each symbol (AAPL, MSFT, GOOGL, TSLA, NVDA):
    for symbol in symbols:
        
        # A. Fetch from yfinance
        ticker = yf.Ticker(symbol)
        hist = ticker.history(period='1d')
        latest = hist.iloc[-1]  # Most recent price
        
        # B. Store in PostgreSQL
        price_record = PriceData(
            symbol=symbol,
            timestamp=now(),
            open=latest['Open'],
            high=latest['High'],
            low=latest['Low'],
            close=latest['Close'],
            volume=latest['Volume'],
            source='yfinance'
        )
        session.add(price_record)
        session.commit()
        
        # C. Cache in Redis (fast access)
        redis.hset(
            f"price:{symbol}:latest",
            {'close': latest['Close'], 'timestamp': now()}
        )
        redis.expire(f"price:{symbol}:latest", 60)
        
        # D. Update Neo4j graph
        neo4j.run("""
            MERGE (s:Stock {symbol: $symbol})
            SET s.last_price = $price,
                s.last_updated = $timestamp
        """, symbol=symbol, price=latest['Close'])
    
    # === INGESTION CYCLE END ===
    
    # Log metrics
    logger.info(f"Cycle complete: {processed}/{total}")
    
    # Wait 60 seconds
    await asyncio.sleep(60)
```

---

## ğŸ¯ Data Flow Architecture

### Input Sources
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Sources   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. yfinance     â”‚ â† Currently active (free)
â”‚ 2. Polygon.io   â”‚ â† Ready (API key in .env)
â”‚ 3. Alpha Vantageâ”‚ â† Ready (6 API keys in .env)
â”‚ 4. Finnhub      â”‚ â† Ready (API key in .env)
â”‚ 5. FMP          â”‚ â† Ready (API key in .env)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Storage Targets (Multi-Database)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE ARCHITECTURE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  PostgreSQL    â”‚    â”‚    Redis      â”‚       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚  â”‚ â€¢ price_data   â”‚    â”‚ â€¢ Latest      â”‚       â”‚
â”‚  â”‚ â€¢ fundamentals â”‚    â”‚   prices      â”‚       â”‚
â”‚  â”‚ â€¢ time series  â”‚    â”‚ â€¢ <1ms access â”‚       â”‚
â”‚  â”‚ â€¢ ACID         â”‚    â”‚ â€¢ TTL: 60s    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    Neo4j       â”‚    â”‚  ChromaDB     â”‚       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚  â”‚ â€¢ Stock nodes  â”‚    â”‚ â€¢ Embeddings  â”‚       â”‚
â”‚  â”‚ â€¢ Sectors      â”‚    â”‚ â€¢ Semantic    â”‚       â”‚
â”‚  â”‚ â€¢ Relationshipsâ”‚    â”‚   search      â”‚       â”‚
â”‚  â”‚ â€¢ Graph queriesâ”‚    â”‚ â€¢ Similarity  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Current Pipeline Configuration

### Environment Variables
```bash
# What symbols to track
SYMBOLS=AAPL,MSFT,GOOGL,TSLA,NVDA

# How often to run (seconds)
PIPELINE_INTERVAL=60

# Which data source
DATA_SOURCE=yfinance  # Free, unlimited

# Database connections (from .env)
POSTGRES_HOST=postgres
POSTGRES_USER=axiom
POSTGRES_PASSWORD=axiom_secure_2024
REDIS_HOST=redis
NEO4J_URI=bolt://neo4j:7687
```

### Execution Pattern
```
Container starts
    â†“
Initialize databases connections
    â†“
Enter infinite loop:
    â”œâ”€ Fetch data for 5 symbols
    â”œâ”€ Store in PostgreSQL
    â”œâ”€ Cache in Redis
    â”œâ”€ Update Neo4j
    â”œâ”€ Log metrics
    â”œâ”€ Sleep 60 seconds
    â””â”€ Repeat
```

---

## ğŸš€ Scaling to Multiple Pipelines

### Option 1: Add More Services to docker-compose.yml

```yaml
services:
  # Current: Real-time prices
  realtime-prices:
    container_name: axiom-pipeline-realtime
    ...

  # NEW: Historical backfill
  historical-backfill:
    container_name: axiom-pipeline-historical
    build:
      dockerfile: axiom/pipelines/Dockerfile.historical
    environment:
      SYMBOLS: AAPL,MSFT,GOOGL,TSLA,NVDA,SPY,QQQ
      LOOKBACK_YEARS: 5
      INTERVAL: 3600  # Run once per hour
    ...

  # NEW: Fundamentals scraper
  fundamentals:
    container_name: axiom-pipeline-fundamentals
    build:
      dockerfile: axiom/pipelines/Dockerfile.fundamentals
    environment:
      SYMBOLS: AAPL,MSFT,GOOGL
      INTERVAL: 86400  # Daily
    ...

  # NEW: Options chain
  options-chain:
    container_name: axiom-pipeline-options
    build:
      dockerfile: axiom/pipelines/Dockerfile.options
    environment:
      SYMBOLS: SPY,QQQ,AAPL
      INTERVAL: 300  # 5 minutes
    ...
```

### Option 2: Horizontal Scaling (Multiple Instances)

```yaml
services:
  # Scale by symbol groups
  ingestion-tech:
    environment:
      SYMBOLS: AAPL,MSFT,GOOGL,NVDA,META
  
  ingestion-finance:
    environment:
      SYMBOLS: JPM,GS,MS,BAC,C
  
  ingestion-energy:
    environment:
      SYMBOLS: XOM,CVX,COP,SLB
```

### Option 3: Kubernetes Deployment (Future)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: axiom-pipeline-ingestion
spec:
  replicas: 3  # Multiple instances
  selector:
    matchLabels:
      app: axiom-pipeline
  template:
    spec:
      containers:
      - name: ingestion
        image: pipelines-data-ingestion:latest
        env:
        - name: SYMBOLS
          value: "AAPL,MSFT,GOOGL,TSLA,NVDA"
```

---

## ğŸ“Š Current Single-Container Justification

### Why 1 Container is Sufficient Now:

1. **Simplicity**: Easier to manage, debug, monitor
2. **Resource Efficient**: One container handles 5 symbols easily
3. **Low Volume**: 5 symbols Ã— 60s interval = low load
4. **Unified Workflow**: Same pattern for all symbols
5. **Cost Effective**: Minimal compute resources needed

### When to Scale to Multiple Containers:

1. **High Volume**: Tracking 100+ symbols
2. **Different Frequencies**: Some 1-min, some 5-min, some hourly
3. **Different Sources**: Mixing free + paid APIs with rate limits
4. **Resource Isolation**: GPU pipelines vs CPU pipelines
5. **Fault Isolation**: Critical symbols in separate containers

---

## ğŸ”® Future Pipeline Architecture

### Proposed Multi-Pipeline Design:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PIPELINE ORCHESTRATOR                      â”‚
â”‚            (Manages all pipeline containers)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚              â”‚
        â–¼                 â–¼                 â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REALTIME     â”‚  â”‚  HISTORICAL  â”‚  â”‚ FUNDAMENTALS â”‚  â”‚   OPTIONS    â”‚
â”‚   PRICES      â”‚  â”‚   BACKFILL   â”‚  â”‚   SCRAPER    â”‚  â”‚    CHAIN     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 5 symbols   â”‚  â”‚ â€¢ 100 symbolsâ”‚  â”‚ â€¢ 50 companiesâ”‚  â”‚ â€¢ 20 symbols â”‚
â”‚ â€¢ 60s intervalâ”‚  â”‚ â€¢ 5 years    â”‚  â”‚ â€¢ Daily      â”‚  â”‚ â€¢ 5min       â”‚
â”‚ â€¢ yfinance    â”‚  â”‚ â€¢ Polygon    â”‚  â”‚ â€¢ Alpha Vant â”‚  â”‚ â€¢ Polygon    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  STORAGE LAYER       â”‚
              â”‚  (4 Databases)       â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ â€¢ PostgreSQL         â”‚
              â”‚ â€¢ Redis              â”‚
              â”‚ â€¢ Neo4j              â”‚
              â”‚ â€¢ ChromaDB           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– Current Workflow Detailed Explanation

### Container: `axiom-pipeline-ingestion`

**Purpose**: Continuous real-time market data ingestion

**Technology Stack**:
- Python 3.13
- SQLAlchemy (PostgreSQL ORM)
- redis-py (Redis client)
- neo4j-python-driver (Neo4j client)
- yfinance (Yahoo Finance API)

**Execution Flow**:

#### Phase 1: Initialization (Once)
```python
1. Connect to PostgreSQL
   â””â”€ Create price_data table if not exists
   
2. Connect to Redis
   â””â”€ Test connection with ping
   
3. Connect to Neo4j
   â””â”€ Verify authentication

4. Log initialization status
   â””â”€ Report which databases connected
```

#### Phase 2: Continuous Loop (Forever)
```python
Loop every 60 seconds:
    
    For each symbol in [AAPL, MSFT, GOOGL, TSLA, NVDA]:
        
        Step A: Fetch Data
        â”œâ”€ Call yfinance API
        â”œâ”€ Request 1-day history
        â””â”€ Extract latest OHLCV
        
        Step B: Validate
        â”œâ”€ Check data not empty
        â”œâ”€ Validate price format
        â””â”€ Skip if invalid
        
        Step C: Transform
        â”œâ”€ Convert to Decimal (precision)
        â”œâ”€ Add timestamp
        â””â”€ Add source tag
        
        Step D: Store in PostgreSQL
        â”œâ”€ Create PriceData record
        â”œâ”€ Add to session
        â””â”€ Commit transaction
        
        Step E: Cache in Redis
        â”œâ”€ Set key: price:{symbol}:latest
        â”œâ”€ Store: {close, timestamp}
        â””â”€ Expire: 60 seconds
        
        Step F: Update Neo4j
        â”œâ”€ MERGE Stock node
        â”œâ”€ SET last_price, last_updated
        â””â”€ Build graph relationships
        
        Step G: Log Success
        â””â”€ "âœ… AAPL: $150.25"
    
    End loop
    
    Step H: Report Metrics
    â”œâ”€ Symbols processed: 5/5
    â”œâ”€ Records stored: 5
    â”œâ”€ Records cached: 5
    â””â”€ Errors: []
    
    Step I: Sleep
    â””â”€ await asyncio.sleep(60)
    
    Repeat from Step A
```

---

## ğŸ” Data Examples

### PostgreSQL Storage
```sql
SELECT * FROM price_data ORDER BY timestamp DESC LIMIT 5;

| id | symbol | timestamp           | open   | high   | low    | close  | volume     | source   |
|----|--------|---------------------|--------|--------|--------|--------|------------|----------|
| 1  | AAPL   | 2025-11-15 03:00:00 | 150.20 | 151.50 | 149.80 | 150.25 | 52000000   | yfinance |
| 2  | MSFT   | 2025-11-15 03:00:00 | 380.50 | 382.00 | 379.00 | 381.75 | 25000000   | yfinance |
| 3  | GOOGL  | 2025-11-15 03:00:00 | 140.10 | 141.00 | 139.50 | 140.80 | 18000000   | yfinance |
```

### Redis Cache
```bash
redis-cli> HGETALL price:AAPL:latest
1) "close"
2) "150.25"
3) "timestamp"
4) "2025-11-15T03:00:00.123456"

redis-cli> TTL price:AAPL:latest
(integer) 45  # Seconds until expiration
```

### Neo4j Graph
```cypher
MATCH (s:Stock {symbol: 'AAPL'})
RETURN s.symbol, s.last_price, s.last_updated

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ symbol  â”‚ last_price  â”‚ last_updated         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 'AAPL'  â”‚ 150.25      â”‚ 2025-11-15T03:00:00  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Why This Design?

### Single Container Advantages:
1. **Atomic Operations**: All or nothing - data consistency
2. **Simplified Monitoring**: One container to watch
3. **Unified Logging**: All logs in one place
4. **Lower Overhead**: Minimal resource usage
5. **Easier Debugging**: Single failure point

### When to Use Multiple Containers:
1. **Different Data Sources**: Some need authentication, some free
2. **Different Intervals**: Real-time (1s) vs batch (1h)
3. **Resource Isolation**: CPU vs GPU pipelines
4. **Fault Tolerance**: Critical vs non-critical data
5. **Scaling**: 1000+ symbols need parallel processing

---

## ğŸ’¡ Recommendations

### For Current Scale (5 symbols):
âœ… **Keep 1 container** - perfectly adequate

### To Add More Pipelines:

1. **Create new pipeline scripts**:
```bash
axiom/pipelines/
â”œâ”€â”€ lightweight_data_ingestion.py     # Current (real-time)
â”œâ”€â”€ historical_backfill.py            # New (batch)
â”œâ”€â”€ fundamentals_scraper.py           # New (daily)
â””â”€â”€ options_chain_ingestion.py        # New (5-min)
```

2. **Add services to docker-compose.yml**:
```yaml
services:
  realtime-ingestion:    # Current
  historical-backfill:   # Add this
  fundamentals-scraper:  # Add this
  options-chain:         # Add this
```

3. **Deploy**:
```bash
docker compose -f axiom/pipelines/docker-compose.yml up -d
```

---

## ğŸ“ Quick Reference

### Current Pipeline Status
```bash
docker ps --filter "name=pipeline"
# axiom-pipeline-ingestion   Up X minutes (healthy)
```

### View Workflow in Action
```bash
docker logs -f axiom-pipeline-ingestion
# Shows each step: Fetch â†’ Store â†’ Cache â†’ Update â†’ Log
```

### Container Configuration
- **File**: `axiom/pipelines/docker-compose.yml`
- **Script**: `axiom/pipelines/lightweight_data_ingestion.py`
- **Networks**: axiom_network + database_axiom_network
- **Restart Policy**: unless-stopped

---

**The pipeline IS running. It's a single unified container by design, not a limitation.**